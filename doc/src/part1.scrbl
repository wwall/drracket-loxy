#lang scribble/base
@(require scribble/core)
@(require racket/sandbox
          scribble/eval)
@(require scribble/manual)
@(require "../../lang/lexer/lexer.scm")
@(require "../../lang/lexer/keywords.scm")
@(define (red-text . s)
   @elem[s #:style (style #f (list (color-property "red")))]) 
@(define (todo . s)
   (let [(text (list (string-append "TODO " (car s))))]
     (red-text text))) 




@title[#:version "0.1"
       #:date "2014-11-24"]{Разработка статического анализатора языка 1С}
@(author+email "Бомбин Валентин" "wwall@yandex.ru" #:obfuscate? #t)
@;----
@local-table-of-contents[]

@section{Введение}
@subsection{Благодарности}
@para{Эта статья никогда не увидела бы свет, если бы не поддержка моей жены - Людмилы. Мой ласковый ангел, эту статью я посвящаю тебе и твоему терпению.}
@subsection{О чем пойдет речь?}
@para{Если вам часто приходиться анализировать чужой код или оценивать чужие разработки, то рано или поздно возникнет идея автоматизировать собственную работу. Вот именно этой автоматизации и посвящена эта работа}
@section{Как все это будет работать}
@para{Сперва разработаем лексер и примитивные правила, затем парсер и чуть более сложные правила, в итоге - подключим модуль пролога и сведем все воедино. Попутно разработаем модуль учета сообщений.}
@subsection{Разбор строк на токены}
@para{Первая задача которую надо решить - разбитие входно строки на токены. Для этого напишем функцию построения лексера и обертку над ней - преобразователь в список}
@para{В модуле lexer.scm представлена приблизительная реализация. Пример работы - ниже - }

@interaction[
 (require "../../lang/lexer/lexer.scm") 
 (string->token-list "функция а() экспорт
возврат 1;
конецфункции") ] 

@para{Как видим строка разбита на последовательность токенов которые мы теперь можем анализировать. Теперь мы можем разработать какое-нибудь  простое правило. Для примера возьмем правило вычисляющее LOC функции.
 Логика работы функции следующая - бежим по списку, когда встречаем токен с типом 'lxmFUNC запоминаем позицию токена и следующее за ним имя. Продолжам сканировать список пока не встретим 'lxmENDFUNC и вычисляем разность строк по токенам. Полученый результат регистрируем в списке результатов.}


@interaction[
 (require "../../lang/lexer/lexer.scm") 
 (require "../../rules/lexer-rules.rkt")
 (for-each
  (lambda (x)
    (printf "name: ~a, LOC: ~a~n" (token-value (structure-rule-loc-token x)) (structure-rule-loc-size x))) (rule-LOC (file->token-list "../../test-data/loc-file-test.txt") '()))] 



@section{Что дальше?}

