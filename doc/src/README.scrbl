#lang scribble/base
@(require scribble/core)
@(require racket/sandbox
          scribble/eval)
@(require scribble/manual)




@title[#:version "0.1"
       #:date "2014-11-24"]{Разработка статического анализатора языка 1С}
@(author+email "Бомбин Валентин" "wwall@yandex.ru" #:obfuscate? #t)
@;----
@local-table-of-contents[]

@section{Введение}
@subsection{Благодарности}
@para{Эта статья никогда не увидела бы свет, если бы не поддержка моей жены - Людмилы. Мой ласковый ангел, вот это все я посвящаю тебе и твоему терпению.}
@subsection{О чем пойдет речь?}
@para{Если вам часто приходиться анализировать чужой код или оценивать чужие разработки, то рано или поздно возникнет идея автоматизировать собственную работу. Вот именно этой автоматизации и посвящена эта работа}
@para{Как указывают некоторые источники (тут сылка на Совершненный Код Макконела) программист тратит до 60% своего рабочего времени на чтение кода.
 Определенные стандарты могут облегчить это чтение. Для 1С определен стандарт на ИТС (тут ссылка на ИТС). Многие программисты ему не следуют.
 Одна и целей этой разработки - автоматизирвать контроль за соблюдением стандарта.
 Вторая задача которая решается этой обработкой - разработка собственного набора правил (если он необходим).
 Третья - построение метрик кода (тут ссылка на метрики кода из википедии и стаьтб с хабра, где про них расказывается).
 Четвертая - построение языка запросов к коду, позволяющему проверять определенные пользователем утверждения о коде.
 Пятая - построение графа связей метаданных и кода.
 Описанию как решаются эти задачи и посвящена данная разработка. Ну и как обычно - все это ыв используете на свой страх и риск.}

@para{Рассмотрим код на языке 1С.}


@para{Обоснование выбора лиспа - не хочу заморачиваться с указателями, классами и прочими стандартными техниками. Хочется верить что вы также разделяете мою точку зрения. Если нет - вы можете переписать этот код на любом удобном для вас языке.}
@section{Как все это будет работать}
@para{Сперва разработаем лексер и примитивные правила, затем парсер и чуть более сложные правила, в итоге - подключим модуль пролога и сведем все воедино. Попутно разработаем модуль учета сообщений.}
@subsection{Разбор строк на токены}
@para{Первая задача которую надо решить - разбитие входно строки на токены. Для этого напишем функцию построения лексера и обертку над ней - преобразователь в список}
@para{В модуле lexer.scm представлена приблизительная реализация. Пример работы - ниже - }

@interaction[
 (require "../../lang/lexer/lexer.scm") 
 (string->token-list "функция а() экспорт
возврат 1;
конецфункции") ] 

@para{Как видим строка разбита на последовательность токенов которые мы теперь можем анализировать. Теперь мы можем разработать какое-нибудь  простое правило. Для примера возьмем правило вычисляющее LOC функции.
 Логика работы функции следующая - бежим по списку, когда встречаем токен с типом 'lxmFUNC запоминаем позицию токена и следующее за ним имя. Продолжам сканировать список пока не встретим 'lxmENDFUNC и вычисляем разность строк по токенам. Полученый результат регистрируем в списке результатов.}


@interaction[
 (require "../../lang/lexer/lexer.scm") 
 (require "../../rules/lexer-rules.rkt")
 (for-each
  (lambda (x)
    (printf "name: ~a, LOC: ~a~n" (token-value (structure-rule-loc-token x)) (structure-rule-loc-size x))) (rule-LOC (file->token-list "../../test-data/loc-file-test.txt") '()))] 



@section{Что дальше?}

