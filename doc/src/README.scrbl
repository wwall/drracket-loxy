#lang scribble/base
@(require scribble/core)
@(require racket/sandbox
          scribble/eval)
@(require scribble/manual)




@title[#:version "0.1"
       #:date "2014-11-24"]{Разработка статического анализатора языка 1С}
@(author+email "Бомбин Валентин" "wwall@yandex.ru" #:obfuscate? #t)
@;----
@local-table-of-contents[]

@section{Введение}
@subsection{Благодарности}
@para{Эта статья никогда не увидела бы свет, если бы не поддержка моей жены - Людмилы. Мой ласковый ангел, вот это все я посвящаю тебе и твоему терпению.}
@subsection{О чем пойдет речь?}
@para{Если вам часто приходиться анализировать чужой код или оценивать чужие разработки, то рано или поздно возникнет идея автоматизировать собственную работу. Вот именно этой автоматизации и посвящена эта работа}
@para{Как указывают некоторые источники (тут ссылка) программист тратит до 60% своего рабочего времени на чтение кода.
 Определенные стандарты могут облегчить это чтение. Для 1С определен стандарт на ИТС (тут ссылка на ИТС), но как это бывает в нашем мире -  многие программисты ему не следуют.
 Одна и целей этой разработки - автоматизирвать контроль за соблюдением стандарта.
 Вторая задача которая решается этой обработкой - разработка собственного набора правил (если он необходим).
 Третья - построение метрик кода (тут ссылка на метрики кода из википедии и стаьтб с хабра, где про них расказывается).
 Четвертая - построение языка запросов к коду, позволяющему проверять определенные пользователем утверждения о коде.
 Пятая - построение графа связей метаданных и кода.
 Описанию как решаются эти задачи и посвящена данная разработка. Ну и как обычно - все это ыв используете на свой страх и риск.}

@para{Обоснование выбора лиспа - он мне нравится. Хочется верить что вы также разделяете мою точку зрения. Если нет - вы можете переписать этот код на любом удобном для вас языке.}
@section{Как все это будет работать}
@para{Сперва разработаем лексер и примитивные правила, затем парсер и чуть более сложные правила, в итоге - подключим модуль пролога и сведем все воедино. Попутно разработаем модуль учета сообщений.}
@subsection{Разбор строк на токены}
@para{Первая задача которую надо решить - разбитие входно строки на токены. Для этого напишем функцию построения лексера и обертку над ней - преобразователь в список}
@para{В модуле lexer.scm представлена приблизительная реализация. Пример работы - ниже - }

@interaction[
 (require "../../lang/lexer/lexer.scm") 
 (string->token-list "функция а() экспорт
возврат 1;
конецфункции") ] 

@para{Как видим строка разбита на последовательность токенов которые мы теперь можем анализировать. Теперь мы можем разработать какое-нибудь  простое правило. Для примера возьмем правило вычисляющее LOC функции.
 Логика работы функции следующая - бежим по списку, когда встречаем токен с типом 'lxmFUNC запоминаем позицию токена и следующее за ним имя. Продолжам сканировать список пока не встретим 'lxmENDFUNC и вычисляем разность строк по токенам. Полученый результат регистрируем в списке результатов.}

@interaction[
 (require racket)
 (require "../../lang/lexer/lexer.scm")
 (require "../../rules/lexer-interface.rkt")
 (require "../../rules/loc-rules.rkt")
 (require "../../rules/loc-eval.rkt")
 (let ([token-list (file->token-list "../../test-data/loc-file-test.txt")])
   (apply-rules token-list)
   (hash-for-each
    (get-rules)
    (lambda (k v)
      (let ([result (get-result k)])
        (when (list? result)
          (for-each
           (lambda (x)
             (printf "name: ~a, LOC: ~a~n"
                     (third (third x))
                     (third (fourth x))))
           result))))))]


@para{Список уже зарегистриованных правил возвращается функцией list-rules}

@interaction[
 (require "../../lang/lexer/lexer.scm"
          "../../rules/lexer-interface.rkt"
          "../../rules/loc-rules.rkt"
          "../../rules/loc-eval.rkt")
 (list-rules)
                                     
 ]

@section{Что дальше?}

