# Разработка статического анализатора языка 1С

Бомбин Валентин <wwall at yandex dot ru>

    1 Введение                  
      1.1 Благодарности         
      1.2 О чем пойдет речь?    
                                
    2 Как все это будет работать
      2.1 Разбор строк на токены
                                
    3 Что дальше?               

## 1. Введение

### 1.1. Благодарности

Эта статья никогда не увидела бы свет, если бы не поддержка моей жены -
Людмилы. Мой ласковый ангел, вот это все я посвящаю тебе и твоему
терпению.

### 1.2. О чем пойдет речь?

Если вам часто приходиться анализировать чужой код или оценивать чужие
разработки, то рано или поздно возникнет идея автоматизировать
собственную работу. Вот именно этой автоматизации и посвящена эта работа

Как указывают некоторые источники \(тут сылка на Совершненный Код
Макконела\) программист тратит до 60% своего рабочего времени на чтение
кода. Определенные стандарты могут облегчить это чтение. Для 1С
определен стандарт на ИТС \(тут ссылка на ИТС\). Многие программисты ему
не следуют. Одна и целей этой разработки - автоматизирвать контроль за
соблюдением стандарта. Вторая задача которая решается этой обработкой -
разработка собственного набора правил \(если он необходим\). Третья -
построение метрик кода \(тут ссылка на метрики кода из википедии и
стаьтб с хабра, где про них расказывается\). Четвертая - построение
языка запросов к коду, позволяющему проверять определенные пользователем
утверждения о коде. Пятая - построение графа связей метаданных и кода.
Описанию как решаются эти задачи и посвящена данная разработка. Ну и как
обычно - все это ыв используете на свой страх и риск.

Обоснование выбора лиспа - не хочу заморачиваться с указателями,
классами и прочими стандартными техниками. Хочется верить что вы также
разделяете мою точку зрения. Если нет - вы можете переписать этот код на
любом удобном для вас языке.

## 2. Как все это будет работать

Сперва разработаем лексер и примитивные правила, затем парсер и чуть
более сложные правила, в итоге - подключим модуль пролога и сведем все
воедино. Попутно разработаем модуль учета сообщений.

### 2.1. Разбор строк на токены

Первая задача которую надо решить - разбитие входно строки на токены.
Для этого напишем функцию построения лексера и обертку над ней -
преобразователь в список

В модуле lexer.scm представлена приблизительная реализация. Пример
работы - ниже -

```racket
> (require "../../lang/lexer/lexer.scm")                              
eval:1:0: cannot open module file                                     
  module path: #<path:E:\lang\lexer\lexer.scm>                        
  path: E:\lang\lexer\lexer.scm                                       
  system error: Не удается найти указанный файл.; errno=2             
> (string->token-list "функция а() экспорт\nвозврат 1;\nконецфункции")
string->token-list: undefined;                                        
 cannot reference undefined identifier                                
```

Как видим строка разбита на последовательность токенов которые мы теперь
можем анализировать. Теперь мы можем разработать какое-нибудь  простое
правило. Для примера возьмем правило вычисляющее LOC функции. Логика
работы функции следующая - бежим по списку, когда встречаем токен с
типом ’lxmFUNC запоминаем позицию токена и следующее за ним имя.
Продолжам сканировать список пока не встретим ’lxmENDFUNC и вычисляем
разность строк по токенам. Полученый результат регистрируем в списке
результатов.

```racket
> (require "../../lang/lexer/lexer.scm")                                                                                                             
eval:1:0: cannot open module file                                                                                                                    
  module path: #<path:E:\lang\lexer\lexer.scm>                                                                                                       
  path: E:\lang\lexer\lexer.scm                                                                                                                      
  system error: Не удается найти указанный файл.; errno=2                                                                                            
> (require "../../rules/lexer-rules.rkt")                                                                                                            
eval:2:0: cannot open module file                                                                                                                    
  module path: #<path:E:\rules\lexer-rules.rkt>                                                                                                      
  path: E:\rules\lexer-rules.rkt                                                                                                                     
  system error: Не удается найти указанный файл.; errno=2                                                                                            
> (for-each                                                                                                                                          
   (lambda (x)                                                                                                                                       
     (printf "name: ~a, LOC:                                                                                                                         
~a~n" (token-value (structure-rule-loc-token x)) (structure-rule-loc-size x))) (rule-LOC (file->token-list "../../test-data/loc-file-test.txt") '()))
rule-LOC: undefined;                                                                                                                                 
 cannot reference undefined identifier                                                                                                               
```

## 3. Что дальше?

